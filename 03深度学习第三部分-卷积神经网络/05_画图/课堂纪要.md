* 图像数据与边缘检测

  * 卷积等算法上个世纪存在
  * **1979年，Kunihiko Fukushima（福岛邦彦），提出了Neocognitron， 卷积、池化的概念基本形成。**
  * **1986年，Geoffrey Hinton与人合著了一篇论文：Learning representations by back-propagation errors。**
  * 1989年，Yann LeCun提出了一种用反向传导进行更新的卷积神经网络，称为LeNet。
  * 原因：
    * 图片数据量大巨大，在多层神经网络当中效果不好，达到盆景了
  * 感受野
    * 来源：猫的神经细胞的感受机制得来
    * 边缘检测，设计一个过滤器对图片进行观察或者检测

* 卷积神经网络

  * 组成

    * 卷积层

      * 卷积运算过程：需要卷积核
        * 大小：奇数大小的过滤器，1 x 1, 3 x 3, 5 x 5
      * 1、正常卷积核运算，默认移动一个像素
        * **缺点：图像变小、边缘信息丢失**
      * 2、零填充
        * **因为0在**权重乘积和运算中对最终结果不造成影响，也就避免了图片增加了额外的干扰信息。
        * 两种方式：
          * Valid:不填充，结果变小
          * SAME：输出图像与原图相等大小，
            * $$P = \frac{F-1}{2}$$
        * 注意：由于避免零填充不均匀，所以我们需要奇数大小的过滤器
      * 3、步长
        * 如果步长不为默认1的情况下，那么
          * s ：步长
        * ((N+2P−F)/s+1),((N+2P−F)/s+1)
      * 多通道的图片：FIlter必须使用形同的通道数
      * 4、多个卷积核数量
        * 得到的特征图大小没有影响，而数量会变化

    * 卷积层的运算结果

      * 每个Filter：f^{[l]} * f^{[l]} * n_{c}^{[l -1]}f[l]∗f[l]∗nc[l−1]
      * 权重Weights：f^{[l]} * f^{[l]} * n_{c}^{[l -1]} * n_{c}^{[l]}f[l]∗f[l]∗nc[l−1]∗nc[l]
      * **应用激活函数Activations**：a^{[l]} = n_{h}^{[l]},n_{w}^{[l]},n_{c}^{[l]}a[l]=nh[l],nw[l],nc[l]
      * 偏差bias：1 * 1 * 1 * n_{c}^{[l]}1∗1∗1∗nc[l]，通常会用4维度来表示

      之前的式子我们就可以简化成,假设多个样本编程向量的形式

      Z^{[l]} = W^{[l]} * X^{[l-1]} + b^{[l]}Z[l]=W[l]∗X[l−1]+b[l]

      A^{[l]} = g(Z^{[l]})A[l]=g(Z[l])

    * 池化层

      * 两种方式
        * **最大值化**，平均池化
      * 窗口设置
        * 2 * 2 ，2个步长，没有参数
      * 作用
        * 降低了后续网络层的输入维度，缩减模型大小，提高计算速度
        * 提高了Feature Map 的鲁棒性，防止过拟合

    * 全连接层

* 经典分类卷积网络结构

  * 对于大量数据这些场景等等，图片数据量、特征大，直接使用现有的结构去做识别等等
  * 对于一些小业务，可以自己设计卷积网络结构
  * LeNet
    * 两个layer(conv+pool)+ 两层网络+全连接层
    * 大小计算、参数计算掌握熟悉
  * AlexNet
    * 6000万参数量，首次使用Relu函数，droupout+BN
  * 对于卷积网络结构的优化
    * 目的：参数数量变少才能减少计算量，对于AlexNet的优化
    * 网络优化目的：一个方向，减少参数量
  * **VGG参数巨大，GoogleNet参数较少**
  * **Inception模块，造成参数变少**
  * 1 * 1 卷积：
    * 1、对多个通道进行信息整合，通道进行线性回归运算
    * 2、达到 通道数的变化，升维和降维或者维度不变化（通道数），参数数量较少
  * Inception:
    * 目的：
      - 代替人手工去确定到底使用1x1,3x3,5x5还是是否需要max_pooling层，由网络自动去寻找适合的结构。并且节省计算。
    * 特点：使用多种卷积核进行运算合并结果通道数
    * 最终结果28 x 28 x 256
      - **使用更少的参数**，达到跟AlexNet或者VGG同样类似的输出结果
    * 计算量还是太大？参数还是太多？
      * 网络缩小后在扩展，在5 * 5 之前加一个1 * 1 卷积结构

* 案例流程总结

  - 网络构建
  - 两层layer:卷积+激活+池化
    - Z1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding="SAME")
    - A1 = tf.nn.relu(Z1)
    - P1 = tf.nn.max_pool(A1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME")
  - FC层
    - 转换成2维
    - tf.contrib.layers.flatten(P2)
    - Z3 = tf.contrib.layers.fully_connected(P2, 6, activation_fn=None)
    - 需要制定神经元个数

* 迁移学习

  * 定义
    - 迁移学习就是**利用数据、任务或模型之间的相似性，将在旧的领域学习过或训练好的模型，应用于新的领域这样的一个过程。**
  * 任务A-已训练好的，任务B新任务
  * 1、建立自己的网络，在A的基础上，修改最后输出结构，并加载A的模型参数
  * 2、根据数据大小调整
    - 如果B任务数据量小，那么我们可以选择将A模型的所有的层进行freeze(可以通过Tensorflow的trainable=False参数实现)，而剩下的输出层部分可以选择调整参数训练
    - 如果B任务的数据量大，那么我们可以将A中一半或者大部分的层进行freeze,而剩下部分的layer可以进行新任务数据基础上的微调