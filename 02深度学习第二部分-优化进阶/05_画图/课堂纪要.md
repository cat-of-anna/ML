* 多分类（softmax回归）
  * 怎么取解决？通过输出每个类别的概率。神经网络：最后一层的输出神经元个数与总类别个数相等
  * 公式：
  * 交叉熵损失：每个样本：对   1log(P(C))，C代表某个类别，要进行损失求和
    * 求出总的损失的平均值：N个样本损失相加，求N个平均值

* 手写数字识别案例设计
  * 确定网络结构以及形状
    - 第一层参数：输入：x [None, 784] 权重：[784, 64] 偏置[64],输出[None, 64]
    - 第二层参数：输入：[None, 64] 权重：[64, 10] 偏置[10]，输出[None, 10]
  * 流程：
    - 获取数据
      - input_data.read_data_sets("./data/mnist/input_data", one_hot=True)
    - 前向传播：网络结构定义
      - 
    - 损失计算
      - tf.nn.softmax_cross_entropy_with_logits(labels=y,
        ​                                                logits=y_predict)
    - 反向传播：梯度下降优化
      - tf.train.GradientDescentOptimizer(0.1).minimize(loss)
  * 功能完善
    - 准确率计算
      - tf.argmax, tf.equal, tf.reduce_mean
    - 添加Tensorboard观察变量、损失变化
      - tf.summary
      - tf.summary.FileWriter("./tmp/summary/", graph=sess.graph)
      - file_writer.add_summary(summary, i)
    - 训练模型保存、模型存在加载模型进行预测
      - tf.train.Saver
        - save
        - restore

* 案例总结：
  * 当网络当中一些参数调整时，并不一定会得到预期的结果
  * 在一些大型数据上，网络根本效果并不好

* 梯度下降算法相关优化
  * 在巨大的数据集基础上进行训练速度很慢。而优化算法能够帮助我们**快速训练模型，提高计算效率**
  * 问题：
    * 梯度爆炸梯度消失
    * 鞍点问题：减少损失的难度也来自误差曲面中的鞍点，而不是局部最低点。
  * 解决办法
    * 初始化参数策略（第一部分第四节提到）
    * Mini梯度下降法
    * 梯度下降算法的优化
      * 学习率衰减

* Mini-batch方法
  * **批梯度下降法(btach)，即同时处理整个训练集**
    * 数据集比较大的时候，处理速度慢
  * Mini-Batch 梯度下降法（小批量梯度下降法）每次同时处理**固定大小**的数据集。
    * mini-batch 的大小为 1，即是随机梯度下降法（stochastic gradient descent）
  * 影响
    * Batch：
      * 迭代速度慢，训练过程慢
    * SGD（随机）：一个样本
      * 有很多噪声，速度当然非常快，但是在大型数据大型网络中会导致一直在鞍点附近徘徊
  * 总结大小如何选择？
    * 如果训练样本的大小比较小，如m\le2000m≤2000时，选择 batch 梯度下降法；
    * 如果训练样本的大小比较大，选择 Mini-Batch 梯度下降法。为了和计算机的信息存储方式相适应，代码在 mini-batch 大小为 2 的幂次时运行要快一些。典型的大小为2^6, 2^7,2^8,2^926,27,28,29,mini-batch 的大小要符合 CPU/GPU 内存。

* 梯度下降算法内部的优化
  * 指数移动平均值
    * $$\beta$$:参数越大，曲线越平缓
    * 参数越小，曲线月曲折
  * 动量梯度下降
    * 对于每一次更新的梯度，先做一个$$\beta$$运算
    * 梯度变得更加平缓一些，加速我们的学习，不会导致震荡过程变大
  * RMSProp
    * **RMSProp（Root Mean Square Prop）**算法是在对梯度进行**指数加权平均的基础上**，引入平方和平方根
    * 公式中两个地方改变
      * 对摆动限制更大一些
  * Adam
    * 结合了两种方式进行修正过程来达到缓和波动
  * 从学习率敏感性
    * 动量 > RMSProp > **Adam**
  * 目的：**为了限制更新的梯度的波动**
  * 学习率的控制
    * 学习前期学习率较大，接近于最小值，学习率变小一些
    * 指数衰减，常用衰减方式

* 输入的标准化
  * 有点：无论从哪个位置开始迭代，都能以相对较少的迭代次数找到全局最优解，
  * 一定程度减少梯度爆炸等（Z = w1x1+w2x2....）,激活函数

* 深度学习正则化
  * 遇到的问题？？？
  * 过拟合与欠拟合（默认算法的优化技巧没问题，学习率，adam）
    * 过拟合：训练集好，测试集不行，模型参数过于复杂
      * 训练集的数据分布没有代表性
      * 解决办法：数据充分
        * **正则化**
        * 寻找更合适的结构
    * 欠拟合：训练集不好，测试集也不好，模型、数据都有可能
      * 扩大网络规模，如添加隐藏层或者神经元数量
      * 寻找合适的网络架构，使用更大的网络结构，如AlexNet
      * 训练时间更长一些

* 正则化
  * **正则化**，**即在成本函数中加入一个正则化项(惩罚项)，惩罚模型的复杂度，防止网络过拟合**

  * L1正则化

  * L2正则化
    * $$\lambda$$是 一个超参数
    * 公式理解：梯度更新的时候，W多减去了一项值（惩罚项进行导数计算得来的）
    * **权重衰减**

  * 神经网络当中的L2范数：矩阵的范数

    * **弗罗贝尼乌斯范数（Frobenius Norm）**

  * droupout正则化

    * 在训练期间， 随机的对神经网络每一层进行丢弃部分神经元操作。

    * 代码

      * ```python
        # 假设设置神经元保留概率
        keep_prob = 0.8
        # 随机建立一个标记1 or 0的矩阵，表示随机失活的单元，占比20%
        dl = np.random.rand(al.shape[0], al.shape[1]) < keep_prob
        # 让a1对应d1的为0地方结果为0
        al = np.multiply(al, dl)
        
        # 为了测试的时候，每一个单元都参与进来
        al /= keep_prob
        ```

  * droupout：达到权重收缩功能（每个权重不会太大）

  * 防止过拟合

  * 确认模型在数据上没有问题，在开启droupout来防止过拟合

* 正则化其它方式

  * 早停止法
    * 不一直训练，训练到一半停止
  * 数据增强
    * 理解：增加数据的多样性，让数据更加典型一些
    * 两种类别
      * 离线增强。预先进行所有必要的变换，从根本上增加数据集的规模（例如，通过翻转所有图像，保存后数据集数量会增加2倍）。
      * 在线增强，或称为动态增强。可通过对即将输入模型的小批量数据的执行相应的变化，这样同一张图片每次训练被随机执行一些变化操作，相当于不同的数据集了。

* 神经网络调优

  * 网格搜索：对于所有参数一一组合去尝试，观察结果
  * 尽量让每一组差别明显，会涉及到参数
  * 学习率：跨度尽量大
  * $$\beta$$：尽量在1附近去相近的值，指数增加效应

* BN：批标准化

  * 调参过程麻烦，训练过程太长；

  * 训练深度神经网络很复杂，因为在训练期间每层输入的分布发生变化，因为前一层的参数发生了变化。这通过要求较低的学

    习率和仔细的参数初始化来减慢训练速度，并且使得训练具有饱和非线性的模型变得非常困难

  * 为什么要用?

    * 调优

  * 原理？

    * 在线性计算之后，加入BN计算，4个步骤
      * 计算每一层，计算输出平均值
      * 计算每一层，标准差
      * 标准化结果
      * $$\gamma, \beta$$，网络自动学习，每一层两个参数值不一样

  * BN的为什么能达到效果？

    * 解决了 **在网络当中数据的分布会随着不同数据集改变**
    * **Batch Normalization的作用就是减小Internal Covariate Shift 所带来的影响，让模型变得更加健壮，鲁棒性（Robustness）更强。** 