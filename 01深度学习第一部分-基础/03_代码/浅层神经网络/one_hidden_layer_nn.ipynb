{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 题目\n",
    "实现第一个神经网络，一个隐藏层。\n",
    "要求\n",
    "* 实现具有单个隐藏层的2类分类神经网络\n",
    "* 使用具有非线性激活功能的单位，例如tanh\n",
    "* 计算交叉熵损失\n",
    "* 实现向前和向后传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from load_dataset import load_planar_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取数据集\n",
    "获取数据集的API已经有不需要实现，数据有两个类别标签y = 0，y = 1的点组成，每个点有两个特征。建立一个模型来拟合这个数据。\n",
    "\n",
    "<img src=\"images/数据集.png\" style=\"width:600px;height:300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = load_planar_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集特征值的形状: (2, 400)\n",
      "数据集目标值的: (1, 400)\n",
      "样本数: 400\n"
     ]
    }
   ],
   "source": [
    "print ('数据集特征值的形状:', X.shape)\n",
    "print ('数据集目标值的:', Y.shape)\n",
    "print ('样本数:', X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单隐层神经网络模型\n",
    "<img src=\"images/classification_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "* 前向传播\n",
    "$$Z^{[1]} = W^{[1]}X+b^{[1]}$$                       \n",
    "\n",
    "$$形状：(4,m) = (4,2) * (2,m) + (4,1)$$\n",
    "\n",
    "$${A}^{[1]}=\\sigma(Z^{[1]})$$                        \n",
    "$$形状：(4,m)$$\n",
    "\n",
    "$$Z^{[2]} = W^{[2]}A^{[1]}+b^{[2]}$$                 \n",
    "$$形状：(1,m) = (1,4) * (4,m)+(1,1)$$\n",
    "\n",
    "$$A^{[2]}=\\sigma(Z^{[2]})$$                          \n",
    "$$形状：(1,m)$$\n",
    "\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
    "\n",
    " 计算损失：\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
    "\n",
    "* 步骤\n",
    "    * 定义网络结构\n",
    "    * 初始化参数\n",
    "    * 循环一下步骤\n",
    "        * 前向传播\n",
    "        * 计算损失\n",
    "        * 反向传播获得梯度\n",
    "        * 梯度更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、定义神经网络结构\n",
    "网络输入输出以及隐藏层神经元个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # 输入层大小\n",
    "    n_x = X.shape[0]\n",
    "    # 隐层大小\n",
    "    n_h = 4\n",
    "    # 输出层大小\n",
    "    n_y = Y.shape[0]\n",
    "    ### END CODE HERE ###\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、 初始化模型参数\n",
    "随机初始化权重以及偏置为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    输入每层的神经元数量\n",
    "    \n",
    "    返回：隐层、输出层的参数\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2)\n",
    "    \n",
    "    ### 开始\n",
    "    # 创建隐层的两个参数\n",
    "    # 让值小一些\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    \n",
    "    # 创建输出层前对应的参数\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    ### 结束\n",
    "    \n",
    "    # 检测维度是否符合要求\n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、循环中的第一步：前向传播\n",
    "根据之前给的前向传播公式，完成该函数\n",
    "$$Z^{[1]} = W^{[1]}X+b^{[1]}$$\n",
    "\n",
    "$${A}^{[1]}=tanh(Z^{[1]})$$             \n",
    "\n",
    "$$Z^{[2]} = W^{[2]}A^{[1]}+b^{[2]}$$\n",
    "\n",
    "$$A^{[2]}=\\sigma(Z^{[2]})$$            \n",
    "\n",
    "使用的函数：np.dot,np.tanh, np.sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X:(n_feature, m)\n",
    "    \n",
    "    Returns:\n",
    "    A2:最后一层的输出\n",
    "    cache:用于反向传播计算的存储中间计算结果的字典\n",
    "    \"\"\"\n",
    "    # 获取参数\n",
    "    ### 开始\n",
    "    # 取出每一层的参数\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    # 进行一层一层的运算\n",
    "    Z1 = np.matmul(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    \n",
    "    # 第二层\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    ### 结束\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、循环中的二步：计算损失\n",
    "完成损失计算的过程，根据损失公式\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
    "\n",
    "设计多个维度，使用np.multiply进行乘法运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    parameters：最后一层输出，目标值，参数\n",
    "    return:损失\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    ### 开始\n",
    "    logpro = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), (np.log(1 - A2)))\n",
    "    cost = - 1 / m * np.sum(logpro)\n",
    "    \n",
    "    ### 结束\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、循环中的第三步：反向传播\n",
    "反向传播在这个网络中分为两步\n",
    "第一次计算：\n",
    "$$dZ^{[2]} = A^{[2]} - Y$$\n",
    "\n",
    "$$dW^{[2]}=\\frac{1}{m}dZ^{[2]}{A^{[1]}}^{T}$$\n",
    "\n",
    "$$db^{[2]}=\\frac{1}{m}np.sum(dZ^{[2]}, axis=1)$$\n",
    "\n",
    "隐层的反向传播梯度计算：\n",
    "$$dZ^{[1]} = {W^{[2]}}^{T}dZ^{[2]}*{(1-g(Z^{[1]})}^{2}={W^{[2]}}^{T}dZ^{[2]}*{(1-A^{[1]})}^{2}$$\n",
    "\n",
    "$$dW^{[1]}=\\frac{1}{m}dZ^{[1]}X^{T}$$\n",
    "\n",
    "$$db^{[1]} = \\frac{1}{m}np.sum(dZ^{[1]}, axis=1)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    parameters：\n",
    "    cache：存储每层前向传播计算结果\n",
    "    X：数据特征\n",
    "    Y：数据目标值\n",
    "    \n",
    "    return:每个参数的梯度\n",
    "    \"\"\"\n",
    "    # 得出训练样本数量\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # 获取参数和缓存中的输出\n",
    "    ### 开始\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    ### 结束\n",
    "    \n",
    "    # 反向传播计算梯度\n",
    "    ### 开始\n",
    "    # 最后一层的参数梯度计算\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    # 隐藏层的参数梯度计算\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
    "    dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    ### 结束\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、循环中的第四步：更新梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 0.005):\n",
    "    \"\"\"\n",
    "    参数：网络参数，梯度，学习率\n",
    "    返回更新之后的参数\n",
    "    \"\"\"\n",
    "    # 获取参数以及梯度\n",
    "    ### 开始\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    \n",
    "    ## 结束\n",
    "    \n",
    "    # 使用学习率更新参数\n",
    "    ### 开始\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "\n",
    "    ### 结束\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、建立网络模型训练逻辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_model(X, Y, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # 初始化参数\n",
    "    ### 开始\n",
    "    # 获取网络的层大小\n",
    "    # 2, 4, 1\n",
    "    n_x, n_h, n_y = layer_sizes(X, Y)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    ### 结束\n",
    "    \n",
    "    # 循环\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        ### 开始\n",
    "        # 前向传播\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # 计算损失\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    "        \n",
    "        # 反向传播计算梯度\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        \n",
    "        # 利用梯度更新参数\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "        ### 结束\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print (\"迭代次数 %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5、预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # 计算概率值，以及判断类别\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = np.array( [1 if x >0.5 else 0 for x in A2.reshape(-1,1)] ).reshape(A2.shape)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代次数 0: 0.693048\n",
      "迭代次数 1000: 0.692359\n",
      "迭代次数 2000: 0.688928\n",
      "迭代次数 3000: 0.682109\n",
      "迭代次数 4000: 0.677535\n",
      "迭代次数 5000: 0.674585\n",
      "迭代次数 6000: 0.670325\n",
      "迭代次数 7000: 0.661691\n",
      "迭代次数 8000: 0.649615\n",
      "迭代次数 9000: 0.638593\n",
      "准确率: 58%\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "parameters = nn_model(X, Y, num_iterations = 10000)\n",
    "\n",
    "predictions = predict(parameters, X)\n",
    "print ('准确率: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
