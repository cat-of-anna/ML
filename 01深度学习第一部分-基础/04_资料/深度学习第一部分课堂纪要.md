* 机器学习与深度学习的区别
  * 特征处理
    * 机器学习：人工处理设计特征
    * 深度学习：不需要人工处理，只要经过网络输出即可
      * 图像、自然语言处理、语言
* 深度学习应用场景
  * 图像、语言、语音
  * 机器学习效果不行的这些场景，特征需要复杂处理的地方
* 神经网络
  * 输入层
  * 隐藏层
  * 输出层
  * 特点
    * 每个连接都有个权值，同一层神经元之间没有连接
    * 神经元当中会含有**激活函数**
    * 最后的输出结果对应的层也称之为**全连接层**
  * 类似人的神经元的结构
* 逻辑回归
  * $$\hat{y} = \sigma(w^Tx+b)=\sigma(w_1x_1+w_2x_2+...+w0)=\sigma(\theta^Tx)y^=σ(wTx+b)=σ(w1x1+w2x2+...+b)=σ(θTx)$$
  * P(y=1|x)概率值
  * 损失函数：L(y^,y)=−(ylogy^)−(1−y)log(1−y^)
* 梯度下降算法
  * 按照梯度下降方向进行优化损失直到最小
  * w:=w−αdwdJ(w,b)，b := b - \alpha\frac{dJ(w, b)}{db}b:=b−αdbdJ(w,b)
* 导数
  * 在某一点的斜率。斜率：当增加自变量一定不可估计的小的值，应变量增加的相对于自变量的倍数
  * 计算图：
    * 从前往后的计算过程
    * 从后往前的导数过程
    * 链式法则：
      * $$\frac{dJ}{dc}=9=\frac{dJ}{du}\frac{du}{dc}=3*3dcdJ=9=dudJdcdu=3∗3$$
* 逻辑回归的梯度下降
  * $$dw1dJ=dzdJdw1dz=dz∗x1$$
  * $$\frac{dJ}{dw_2} = \frac{dJ}{dz}\frac{dz}{dw_1}=dz*x2dw2dJ=dzdJdw1dz=dz∗x2$$
  * $$\frac{dJ}{db}=dzdbdJ=dz$$
  * 根据梯度下降公式，按照得出来的梯度来更新
  * m个样本
    * 循环m个样本，每个样本计算一次参数的梯度，求和再求平均值，更新
* 向量化编程
  * 避免适用for循环进行向量的计算，适用np.dot等numpy工具进行操作
* 逻辑回归的m个样本的向量化编程
* 前向传播与反向传播
* 实现猫的二分类
  * 初始化参数
  * 迭代N次计算梯度，更新参数
  * 用更新的参数计算预测结果