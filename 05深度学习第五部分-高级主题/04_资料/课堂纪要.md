高级主题

* GAN
* 自动编码器
* CapsuleNet

# 5.1 生成对抗网络(GAN)

### 5.1.1 GAN能做什么

- 生成以假乱真的图片
- 生成视频等

### 5.1.2 什么GAN

* 定义
  * 一个**生成器**G（Generator）和一个**判别器**D（Discriminator）
    * 生成器：输入噪点数据生成固定大小图片
    * 判别器：训练样本，生成的图片进行判别真伪

#### 5.1.2.2 理解

* 学习真实训练样本的概率分布

* 过程

  * 1、建立好整个GAN结构，真实数据与假数据样本分布区别大
  * 2、训练判别器，使得能够区分真假样本，生成器不动
  * 3、训练生成器，使得生成的假样本能够接近真实样本的分布

#### 5.1.2.3 训练损失

- 判别器：相当于一个分类器，判断图片的真伪，二分类问题，使用交叉熵损失

#### 5.1.2.4 G、D结构

* 2014
* 2015
  * 1、判别器D中取出pooling，全部变成卷积、生成器G中使用反卷积（下图）
  * 2、D、G中都增加了BN层
  * 3、去除了所有的全连接层
  * 4、判别器D中全部使用Leaky ReLU，生成器除了最后输出层使用tanh其它层全换成ReLU
* 包含两个卷积网络
  * 判别器：卷积网络
  * 生成器：反卷积网络



### 5.1.3案例：GAN生成手写数字图像

#### 5.1.3.1案例演示与结果显示

#### 5.1.3.2代码步骤流程

- 初始化网络模型结构
  - init_model
  - 判别器：CNN，build_discriminator
  - 生成器：CNN，build_generator
- 训练过程：train（self，epochs，batch_size = 128，save_interval = 50）
  - 训练判别器
  - 训练生成器
  - 生成图片保存
- 代码编写
  - 2、初始化GAN模型结构
    - 建立D判别器CNN结构，初始化判别器训练优化参数
    - 联合建立G生成器CNN结构，初始化生成器训练优化参数
    - build_generator(self):
    - build_discriminator
    - from keras.optimizers import Adam
    - self.discriminator.trainable = False
  - 3、训练D、G
    - 1、加载Mnist数据，处理，目标值建立
    - 2、循环迭代训练
      -  1、训练判别器
      - 2、训练生成器，停止判别器

# 5.2 自动编码器

### 5.2.1 自动编码器什么用

* 数据去噪

### 5.2.1 什么是自动编码器(Autoencoder)

* 自动编码器是一种默认数据的压缩算法
* 原理作用
  * 搭建编码器
  * 搭建解码器
  * 设定一个损失函数，用以衡量由于压缩而损失掉的信息。
    - 编码器和解码器一般都是参数化的方程，并关于损失函数可导，**通常情况是使用神经网络。**

#### 5.2.1.3 类别

- 普通自编码器
  - 编解码网络使用全连接层
- 多层自编码器
- 卷积自编码器
  - 编解码器使用卷积结构
- 正则化自编码器
  - 降噪自编码器

### 5.2.2 Keras快速搭建普通自编码器-基于Mnist手写数字

#### 5.2.2.1 自编码器效果

 5.2.2.2 流程-普通自编码器

- 初始化自编码器结构
  - 32个神经元，784个神经元的解码器
- 训练自编码器
  - 读取Mnist数据，并进行归一化处理以及形状修改
  - 模型进行fit训练
    - 指定迭代次数
    - 指定每批次数据大小
    - 是否打乱数据
    - 验证集合
- 显示自编码前后效果对比

### 5.2.3 基于Mnist手写数字-深度自编码器

* 多个自编码器重叠

### 5.2.4 基于Mnist手写数字-卷积自编码器

卷积编解码结构设计

- 编码器

  - Conv2D(32, (3, 3), activation='relu', padding='same')
  - MaxPooling2D((2, 2), padding='same')
  - Conv2D(32, (3, 3), activation='relu', padding='same')
  - MaxPooling2D((2, 2), padding='same')
  - 输出大小为:Tensor("max_pooling2d_2/MaxPool:0", shape=(?, 7, 7, 32), dtype=float32)

- 解码器:反卷积过程

  - Conv2D(32, (3, 3), activation='relu', padding='same')
  - UpSampling2D((2, 2))
  - Conv2D(32, (3, 3), activation='relu', padding='same')
  - UpSampling2D((2, 2))
  - Conv2D(1, (3, 3), activation='sigmoid', padding='same')
  - 输出大小：Tensor("conv2d_5/Sigmoid:0", shape=(?, 28, 28, 1), dtype=float32)

- 由于修改了模型的输入输出数据形状，所以在训练的地方同样也需要修改（显示的时候数据输入也要修改）

  x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))
  x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))


### 5.2.4 基于Mnist手写数字-降噪自编码器

* 过程
  * 对原始数据添加噪音
  * 随机加上正态分布的噪音
* 自编码器能够去处理添加过噪点的数据到原来的数据

### 5.2.5 总结

- 掌握自动编码器的结构
  - encoder,decoder
  - 全连接层：普通自编码器
  - 多个重叠：深度
  - 卷积自编码器
  - 正则化自编码器
- 掌握正则化自动编码器结构作用



### 5.3.1 CapsuleNet为什么会出现

* CNN的目标不正确
  * 1、CNN对于旋转类型图片不确定
    * 要做的就是数据增强处理
  * 2、CNN对于图片整体结构关系不确定
  * 人的视觉系统会建立坐标框架，坐标框架是参与到识别过程中，识别过程受到了空间概念的支配

### 5.3.2 什么是CapsuleNet

胶囊的结构添加到CNN当中

#### 5.3.2.1 改进特点

*  添加胶囊层
  * Capsule 是一组神经元，其**输入输出向量表示特定实体类型的实例化参数（即特定物体、概念实体等出现的概率与某些属性）**。
  * 输出概率之外还会输出特定类别的实体属性
  * 10 * 16 ，16表示每个类别的属性

#### 5.3.2.2 结构

#### 5.3.2.3 效果

## 课程内容总结

* 深度学习与神经网络
  * 神经网络基础
    * 逻辑回归案例
    * 导数理解
    * 向量化计算
    * 前向传播和反向传播
  * 浅层神经网络
    * 激活函数
    * BP
    * 掌握推导案例
  * 深层网络
    * 了解整个BP过程
* 深度学习进阶
  * 分类
    * 多分类怎么解决
      * softmax
  * 梯度下降算法优化
    * 算法遇到问题
      * 梯度爆炸、消失等问题
    * 算法+移动平均
    * 学习率衰减
    * 标准化输入
  * 正则化
    * 解决过拟合
    * L1、L2
    * Droupout
    * 其它方法
  * 参数调优
    * 参数值设置问题
    * BN
* 卷积网络
  * 卷积网络原理
    * 卷积
    * 池化
    * 计算公式
  * 经典网络结构
    * 通用现有的模型
      * NIN：1 * 1卷积
      * VGG
      * GoogleNet
      * Inception结构
  * CNN
    * 手势识别图像分类案例
* 循环神经网络
  * 对于序列数据处理效果好
  * **循环神经网络**
    * **时间的反向传播**
  * 长时间记忆问题
    * GRU
    * LSTM
  * 词嵌入
    * one_hot
    * word2vec
    * gensim训练案例，得到词向量结果
  * 序列模型与翻译（注意力机制）
    * se2seq:encoder-decoder
    * BLEU打分机制
    * Attention机制：增加一些特征的影响因素
  * 时间日期的翻译
* 高级主题
  * 生成对抗网络
    * 生成器G
    * 判别器D
    * 全连连接层
    * 卷积网络（DCGAN）
  * 自动编码器
    * encoder-decoder
    * 模型
      * 全连接层
      * 深度
      * 卷积
      * 降噪处理（前期数据加上噪点）
  * 胶囊网络